{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35247,"status":"ok","timestamp":1737211054975,"user":{"displayName":"Julius Maliwat","userId":"06100578401494245092"},"user_tz":-60},"id":"tKUGr5pkyH2G","outputId":"c5ea6659-4950-40e0-c152-b42404437fbd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.2/910.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["!pip install gradio --quiet\n","!pip install ultralytics --quiet\n","\n","import gradio as gr\n","from ultralytics import YOLO\n","import cv2\n","from PIL import Image\n","import numpy as np\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6257,"status":"ok","timestamp":1737211061213,"user":{"displayName":"Julius Maliwat","userId":"06100578401494245092"},"user_tz":-60},"id":"HA9EU8l4zBvJ","outputId":"5f53c6bd-a01d-4663-c203-73e1c0a4c9a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading model...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=18ziUupmCZKtv_cBzzWhY1t45Dm27ras9\n","To: /content/best.pt\n","100%|██████████| 6.23M/6.23M [00:00<00:00, 11.2MB/s]"]},{"output_type":"stream","name":"stdout","text":["Model downloaded successfully as best.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import gdown\n","model_url = \"https://drive.google.com/uc?id=18ziUupmCZKtv_cBzzWhY1t45Dm27ras9\"\n","model_path = \"best.pt\"  # Save the file in the current working directory\n","\n","# Download the metadata file\n","print(\"Downloading model...\")\n","gdown.download(model_url, model_path, quiet=False)\n","print(f\"Model downloaded successfully as {model_path}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jCamASG4zJ9P","executionInfo":{"status":"ok","timestamp":1737211061213,"user_tz":-60,"elapsed":5,"user":{"displayName":"Julius Maliwat","userId":"06100578401494245092"}}},"outputs":[],"source":["model = YOLO(\"./best.pt\")  # Path to your saved model\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"7aWD0WVFzOCO","executionInfo":{"status":"ok","timestamp":1737211061213,"user_tz":-60,"elapsed":6,"user":{"displayName":"Julius Maliwat","userId":"06100578401494245092"}}},"outputs":[],"source":["# Function to compute CLAHE parameters dynamically\n","def compute_clahe_parameters(image):\n","    \"\"\"\n","    Compute CLAHE parameters dynamically based on image characteristics.\n","\n","    Parameters:\n","        image (numpy.ndarray): Input image in BGR format.\n","\n","    Returns:\n","        dict: Dictionary with computed `clipLimit` and `tileGridSize`.\n","    \"\"\"\n","    # Convert image to LAB color space\n","    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","    l, _, _ = cv2.split(lab)\n","\n","    # Calculate mean brightness of the L channel\n","    mean_luminance = np.mean(l)\n","\n","    # Adjust parameters based on mean luminance\n","    if mean_luminance < 38:  # Very dark image\n","        clipLimit = 4.0\n","        tileGridSize = (8, 8)\n","    elif mean_luminance < 75:  # Moderately dark image\n","        clipLimit = 3.0\n","        tileGridSize = (8, 8)\n","    else:  # Bright image\n","        clipLimit = 2.0\n","        tileGridSize = (16, 16)\n","\n","    return {\"clipLimit\": clipLimit, \"tileGridSize\": tileGridSize}\n","\n","# Function to apply CLAHE preprocessing\n","def apply_clahe(image):\n","    \"\"\"\n","    Apply CLAHE preprocessing to an input image.\n","\n","    Parameters:\n","        image (numpy.ndarray): Input image in BGR format.\n","\n","    Returns:\n","        numpy.ndarray: Preprocessed image with CLAHE applied.\n","    \"\"\"\n","    # Convert to LAB color space\n","    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","    l, a, b = cv2.split(lab)\n","\n","    # Compute CLAHE parameters\n","    clahe_params = compute_clahe_parameters(image)\n","\n","    # Apply CLAHE to the L channel\n","    clahe = cv2.createCLAHE(clipLimit=clahe_params[\"clipLimit\"], tileGridSize=clahe_params[\"tileGridSize\"])\n","    l_clahe = clahe.apply(l)\n","\n","    # Merge channels back and convert to BGR\n","    lab_clahe = cv2.merge((l_clahe, a, b))\n","    enhanced_image = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n","\n","    return enhanced_image"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XqNpzgfMwVPe","executionInfo":{"status":"ok","timestamp":1737211061214,"user_tz":-60,"elapsed":5,"user":{"displayName":"Julius Maliwat","userId":"06100578401494245092"}}},"outputs":[],"source":["# Inference function\n","def predict(image):\n","    \"\"\"\n","    Run inference on the uploaded image and return the annotated image.\n","\n","    Parameters:\n","        image (PIL.Image): Input image uploaded by the user.\n","\n","    Returns:\n","        PIL.Image: Annotated image with bounding boxes.\n","    \"\"\"\n","    # Convert PIL image to OpenCV format (BGR)\n","    img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","\n","    # Apply CLAHE preprocessing\n","    preprocessed_img = apply_clahe(img)\n","\n","    # Perform prediction\n","    results = model.predict(source=preprocessed_img, save=False, conf=0.25)\n","\n","    # Get the annotated image\n","    annotated_frame = results[0].plot()  # Retrieve the first processed image\n","\n","    # Convert back to PIL format for Gradio\n","    return Image.fromarray(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":834},"id":"URpWsaDnzetA","outputId":"01275d32-67e7-4a97-927c-59e8b2ba17f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://41f218b66d46d7bae7.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://41f218b66d46d7bae7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","0: 448x640 8 Cars, 3 Peoples, 36.9ms\n","Speed: 3.2ms preprocess, 36.9ms inference, 770.5ms postprocess per image at shape (1, 3, 448, 640)\n","\n","0: 448x640 1 Car, 5 Peoples, 1 Table, 7.9ms\n","Speed: 6.8ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","0: 448x640 8 Cars, 3 Peoples, 38.7ms\n","Speed: 2.4ms preprocess, 38.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","0: 448x640 1 Car, 5 Peoples, 1 Table, 6.8ms\n","Speed: 3.3ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n"]}],"source":["# Gradio Interface\n","demo = gr.Interface(\n","    fn=predict,  # Inference function\n","    inputs=gr.Image(type=\"pil\"),  # Input: PIL image\n","    outputs=gr.Image(type=\"pil\"),  # Output: Annotated image\n","    title=\"YOLOv8 Object Detection Demo with CLAHE Preprocessing\",  # Title of the demo\n","    description=\"Upload an image, and the model will display detected objects with bounding boxes. CLAHE preprocessing is applied to the image.\",  # Description\n",")\n","\n","# Launch the demo\n","demo.launch(debug=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMo11ysGD5JpteF+TxolIWb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}